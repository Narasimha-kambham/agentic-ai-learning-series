{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKNNW0EjPLb/Hg71yiPY46"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üåü **LlamaIndex Basics: Core Concepts, Plugins & a Quick 5-Line RAG Demo**\n",
        "\n",
        "This notebook gives a simple introduction to **LlamaIndex**, focusing on how it differs from LangChain/LangGraph and how to quickly use it for small RAG tasks.\n",
        "\n",
        "It covers the essentials:\n",
        "\n",
        "* how core vs plugin packages work\n",
        "* fixing the async issue in Google Colab\n",
        "* making basic LLM calls with Gemini\n",
        "* and running a tiny ‚Äú5-Line RAG‚Äù demo to understand the workflow\n",
        "\n",
        "This notebook is meant to **explore the basics**, not to replace LangChain/LangGraph.\n",
        "\n",
        "<br>\n",
        "\n",
        "## üìù **Note**\n",
        "\n",
        "This notebook is kept intentionally light.\n",
        "The goal is to see *how LlamaIndex works*, not to dive deeply into its architecture or advanced features."
      ],
      "metadata": {
        "id": "fHvfnO7C3baj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LlamaIndex Core and Plugin\n",
        "\n",
        "LlamaIndex uses a core package and optional plugins.  \n",
        "Plugins add extra features to the same import path instead of creating new ones.  \n",
        "So, even if a plugin is installed separately, it is still imported through `llama_index`.\n",
        "\n",
        "### Core vs Plugin\n",
        "\n",
        "| Type     | Install With Pip                  | Import Path                    |\n",
        "|----------|------------------------------------|---------------------------------|\n",
        "| Core     | `llama-index`                     | `llama_index`                  |\n",
        "| Plugin   | `llama-index-llms-google-genai`   | Extends `llama_index.llms`     |\n",
        "\n",
        "### Summary\n",
        "\n",
        "> The core gives the main features, and the plugin adds new features to the same namespace.\n"
      ],
      "metadata": {
        "id": "uRefdbPbEyHN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_eCRI7iKUBs"
      },
      "outputs": [],
      "source": [
        "%pip install llama-index-llms-google-genai llama-index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "3eglCNDVLm4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üîß Understanding the `asyncio.run()` Error in Google Colab**\n",
        "\n",
        "When running LlamaIndex inside Google Colab, you may encounter the following error:\n",
        "\n",
        "*`RuntimeError: asyncio.run() cannot be called from a running event loop`*\n",
        "\n",
        "\n",
        "### **üìå Why This Happens**\n",
        "Google Colab already runs its **own asynchronous event loop** in the background.  \n",
        "LlamaIndex internally tries to start **another** event loop using `asyncio.run()`.  \n",
        "Since Python does not allow starting a new event loop while one is already active, the error is raised.\n",
        "\n",
        "### **‚úîÔ∏è Solution**\n",
        "Apply `nest_asyncio` to allow nested event loops:\n",
        "\n",
        "```python\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n"
      ],
      "metadata": {
        "id": "IVkFPPz6Fu7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "MWqXlb4LFwLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.google_genai import GoogleGenAI\n",
        "\n",
        "llm = GoogleGenAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    api_key = api_key\n",
        ")\n",
        "\n",
        "resp = llm.complete(\"Who is Paul Graham?\")\n",
        "print(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8VJoaQ8NiUn",
        "outputId": "084c6eb9-46ce-4fa6-aa0c-0af3f53949ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paul Graham is a prominent American computer scientist, programmer, essayist, entrepreneur, and venture capitalist. He is best known as the co-founder of **Y Combinator**, one of the world's most influential startup accelerators.\n",
            "\n",
            "Here's a breakdown of his key contributions and roles:\n",
            "\n",
            "1.  **Co-founder of Y Combinator (YC):**\n",
            "    *   In 2005, he co-founded Y Combinator with Jessica Livingston (his wife), Robert Morris, and Trevor Blackwell.\n",
            "    *   YC revolutionized startup funding and mentorship by providing seed funding, advice, and a powerful network to thousands of startups in batches.\n",
            "    *   It has funded and mentored some of the most successful tech companies in the world, including **Airbnb, Dropbox, Stripe, Reddit, Coinbase, DoorDash, and many more.**\n",
            "    *   Graham stepped down from day-to-day management of YC in 2014 but remains a partner and advisor.\n",
            "\n",
            "2.  **Entrepreneur (Viaweb):**\n",
            "    *   Before YC, Graham co-founded **Viaweb** in 1995 with Robert Morris.\n",
            "    *   Viaweb was one of the first web-based applications for building online stores, written in the Lisp programming language.\n",
            "    *   It was acquired by Yahoo! in 1998 for $49 million in stock, becoming Yahoo! Store. This successful exit provided him with the capital and experience to later launch Y Combinator.\n",
            "\n",
            "3.  **Programmer and Lisp Advocate:**\n",
            "    *   Graham is a strong advocate for the Lisp programming language. His early work and writings often explored the power and elegance of Lisp.\n",
            "    *   He authored the influential book **\"On Lisp\"** (1993), which is a classic in the Lisp community.\n",
            "\n",
            "4.  **Essayist and Author:**\n",
            "    *   Paul Graham is a prolific and highly influential essayist. His essays, published on his website (paulgraham.com), cover a wide range of topics including startups, programming, technology, culture, wealth, and more.\n",
            "    *   He is known for his clear, insightful, and often provocative style, which has shaped the thinking of countless entrepreneurs, programmers, and technologists.\n",
            "    *   Many of his essays are collected in his books, most notably **\"Hackers & Painters: Big Ideas from the Computer Age\"** (2004).\n",
            "\n",
            "**In summary:** Paul Graham is a pivotal figure in the modern tech and startup world. Through Y Combinator, he helped democratize startup funding and provided a blueprint for accelerator models worldwide. His entrepreneurial success with Viaweb and his widely read essays have also profoundly influenced the culture and philosophy of Silicon Valley and beyond.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(role=\"assistant\",content=\"You are a Haiku writer\"),\n",
        "    ChatMessage(role=\"user\",content=\"Tell me about Ganga river\")\n",
        "]\n",
        "\n",
        "telugu_poem = llm.chat(messages)\n",
        "print(telugu_poem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azck5atwN83e",
        "outputId": "1a0697d9-6d7e-4cbe-ca43-dbd295c3a70b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant: From mountain's high source,\n",
            "Sacred waters gently flow,\n",
            "Life's mother, Ganga.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üöÄ 5-Line RAG (LlamaIndex)**\n",
        "\n",
        "Below, I have implemented the popular **\"5-Line RAG\"** concept using LlamaIndex.  \n",
        "This approach quickly connects our own documents with an AI model to give answers  \n",
        "based on real content rather than guesswork.\n",
        "\n",
        "### üîπ What This 5-Line RAG Does\n",
        "\n",
        "- Loads documents from a folder and prepares them for AI use.\n",
        "- Converts them into vector embeddings for **semantic understanding**.\n",
        "- Builds a quick **in-memory vector index** (no external DB needed for demo).\n",
        "- Uses an LLM to answer questions **based on document context**.\n",
        "- Gives **accurate and relevant replies**, not generic responses.\n",
        "\n",
        "### üîñ Model Used (Important Note)\n",
        "\n",
        "- LlamaIndex **normally defaults to OpenAI** if no model is set.\n",
        "- In this demo, I **manually selected Google GenAI (Gemini)** as the LLM.\n",
        "- The RAG flow remains the **same regardless of model**.\n",
        "- This proves LlamaIndex can work with **any supported LLM provider**.\n",
        "- You can switch models anytime depending on **cost, speed, or accuracy**.\n"
      ],
      "metadata": {
        "id": "W--5VxeSxuJP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3c0d7f2"
      },
      "source": [
        "%pip install llama-index-embeddings-google-genai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63f09904",
        "outputId": "5e28f91b-940c-47ab-cec5-77c19002c81b"
      },
      "source": [
        "import llama_index.embeddings.google_genai\n",
        "print(dir(llama_index.embeddings.google_genai))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['GoogleGenAIEmbedding', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'base']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
        "\n",
        "embedding_model = GoogleGenAIEmbedding(\n",
        "    model_name=\"gemini-embedding-001\",\n",
        "    api_key=api_key\n",
        ")"
      ],
      "metadata": {
        "id": "FJH3iiFea2Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents, embed_model = embedding_model)\n",
        "query_engine = index.as_query_engine(llm=llm)\n",
        "response = query_engine.query(\"How to resign this Company?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4XGEnt0KKgP",
        "outputId": "8c65c224-989c-44f3-c2d2-1ddb513ed584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resignations should be submitted in writing, either through the HRIS or via email. The applicable notice periods are defined by the employee's contract and must align with local law. Management may also consider mutual waivers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ‚úÖ **Summary & Next Steps**\n",
        "\n",
        "In this notebook, you explored the basics of using **LlamaIndex** with Gemini.\n",
        "You learned how to:\n",
        "\n",
        "* Install the core + plugin packages\n",
        "* Fix the Colab async loop issue using `nest_asyncio.apply()`\n",
        "* Run basic LLM calls using `GoogleGenAI`\n",
        "* Use the popular ‚Äú5-Line RAG‚Äù pattern to load documents, embed them, build an index, and query with context\n",
        "\n",
        "This gives you a quick taste of how LlamaIndex works without going deep into its ecosystem.\n",
        "\n",
        "<br>\n",
        "\n",
        "### üîÆ **This prepares you for more advanced notebooks that involve:**\n",
        "\n",
        "* Understanding how to build full multi-agent systems\n",
        "* Working with platforms like CrewAI for agent orchestration\n",
        "* Using structured agents, tasks, variables & memory\n",
        "* Integrating external tools and embeddings into workflows\n",
        "* Managing complex pipelines beyond simple RAG demos\n",
        "\n",
        "<br>\n",
        "\n",
        "üí¨ **Tip:**\n",
        "LlamaIndex is useful for rapid prototyping and small RAG demos, but LangChain/LangGraph give more control for production-like workflows.\n",
        "Use whichever fits the task."
      ],
      "metadata": {
        "id": "HP5q09yn3_1F"
      }
    }
  ]
}